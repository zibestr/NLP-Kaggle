{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "File \u001b[0;32m~/Documents/Python Projects/NLP Kaggle/.venv/lib/python3.11/site-packages/torch/__init__.py:1854\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m library\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m-> 1854\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[0;32m~/Documents/Python Projects/NLP Kaggle/.venv/lib/python3.11/site-packages/torch/_meta_registrations.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     _add_op_to_registry,\n\u001b[1;32m     11\u001b[0m     _convert_out_params,\n\u001b[1;32m     12\u001b[0m     global_decomposition_table,\n\u001b[1;32m     13\u001b[0m     meta_table,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001b[0;32m~/Documents/Python Projects/NLP Kaggle/.venv/lib/python3.11/site-packages/torch/_decomp/__init__.py:244\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecompositions\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_refs\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# See NOTE [Core ATen Ops]\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# list was copied from torch/_inductor/decomposition.py\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# excluding decompositions that results in prim ops\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Resulting opset of decomposition is core aten ops\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcore_aten_decompositions\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[torch\u001b[38;5;241m.\u001b[39m_ops\u001b[38;5;241m.\u001b[39mOperatorBase, Callable]:\n",
      "File \u001b[0;32m~/Documents/Python Projects/NLP Kaggle/.venv/lib/python3.11/site-packages/torch/_refs/__init__.py:1619\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;129m@_make_elementwise_binary_reference\u001b[39m(\n\u001b[1;32m   1612\u001b[0m     type_promotion_kind\u001b[38;5;241m=\u001b[39mELEMENTWISE_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mALWAYS_BOOL,\n\u001b[1;32m   1613\u001b[0m     supports_lhs_python_scalar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1614\u001b[0m )\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlt\u001b[39m(a: TensorLikeType, b: TensorLikeType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorLikeType:\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prims\u001b[38;5;241m.\u001b[39mlt(a, b)\n\u001b[0;32m-> 1619\u001b[0m \u001b[38;5;129;43m@_make_elementwise_binary_reference\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_promotion_kind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mELEMENTWISE_TYPE_PROMOTION_KIND\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmaximum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensorLikeType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensorLikeType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTensorLikeType\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;129m@_make_elementwise_binary_reference\u001b[39m(\n\u001b[1;32m   1627\u001b[0m     type_promotion_kind\u001b[38;5;241m=\u001b[39mELEMENTWISE_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[1;32m   1628\u001b[0m )\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminimum\u001b[39m(a: TensorLikeType, b: TensorLikeType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorLikeType:\n",
      "File \u001b[0;32m~/Documents/Python Projects/NLP Kaggle/.venv/lib/python3.11/site-packages/torch/_refs/__init__.py:1037\u001b[0m, in \u001b[0;36m_make_elementwise_binary_reference.<locals>.inner\u001b[0;34m(prim)\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_noncontiguous_outputs([a, b], output)\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_out:\n\u001b[0;32m-> 1037\u001b[0m     _ref \u001b[38;5;241m=\u001b[39m \u001b[43mout_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m _ref\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aten_op \u001b[38;5;129;01mis\u001b[39;00m infer_aten_op:\n",
      "File \u001b[0;32m~/Documents/Python Projects/NLP Kaggle/.venv/lib/python3.11/site-packages/torch/_prims_common/wrappers.py:306\u001b[0m, in \u001b[0;36mout_wrapper.<locals>._out_wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sig\u001b[38;5;241m.\u001b[39mreturn_annotation, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mreturn_annotation \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    302\u001b[0m     sig\u001b[38;5;241m.\u001b[39mempty,\n\u001b[1;32m    303\u001b[0m     out_type,\n\u001b[1;32m    304\u001b[0m )\n\u001b[1;32m    305\u001b[0m params \u001b[38;5;241m=\u001b[39m chain(sig\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues(), (out_param,))\n\u001b[0;32m--> 306\u001b[0m _fn\u001b[38;5;241m.\u001b[39m__signature__ \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_annotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    308\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m _fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m \u001b[38;5;241m=\u001b[39m fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\n\u001b[1;32m    311\u001b[0m _fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m out_type\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/inspect.py:3002\u001b[0m, in \u001b[0;36mSignature.__init__\u001b[0;34m(self, parameters, return_annotation, __validate_parameters__)\u001b[0m\n\u001b[1;32m   2999\u001b[0m     top_kind \u001b[38;5;241m=\u001b[39m kind\n\u001b[1;32m   3001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;129;01min\u001b[39;00m (_POSITIONAL_ONLY, _POSITIONAL_OR_KEYWORD):\n\u001b[0;32m-> 3002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m _empty:\n\u001b[1;32m   3003\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m seen_default:\n\u001b[1;32m   3004\u001b[0m             \u001b[38;5;66;03m# No default for this parameter, but the\u001b[39;00m\n\u001b[1;32m   3005\u001b[0m             \u001b[38;5;66;03m# previous parameter of had a default\u001b[39;00m\n\u001b[1;32m   3006\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon-default argument follows default \u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m   3007\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margument\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/inspect.py:2743\u001b[0m, in \u001b[0;36mParameter.default\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2739\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   2740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\n\u001b[0;32m-> 2743\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   2744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default\n\u001b[1;32m   2747\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   2748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mannotation\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    'cuda' if torch.cuda.is_available() else\n",
    "    'mps' if torch.backends.mps.is_available() else\n",
    "    'cpu'\n",
    ")\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load('data/train.data')\n",
    "test_dataset = torch.load('data/test.data')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, drop_last=True)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(X.shape, y.shape, X[14], y[14])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, y, train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_vocab = 24426"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, vocab_size: int,\n",
    "                 embedding_dim: int,\n",
    "                 seq_len: int) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                      embedding_dim)  # output shape: (batch_size, seq_len, embedding_dim)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels=seq_len,\n",
    "                          out_channels=1024,\n",
    "                          kernel_size=3),\n",
    "                nn.BatchNorm1d(num_features=1024),\n",
    "                nn.AvgPool1d(kernel_size=1,\n",
    "                             stride=2),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels=1024,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=3),\n",
    "                nn.BatchNorm1d(num_features=512),\n",
    "                nn.AvgPool1d(kernel_size=1,\n",
    "                             stride=2),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels=512,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=3),\n",
    "                nn.BatchNorm1d(num_features=256),\n",
    "                nn.AvgPool1d(kernel_size=1,\n",
    "                             stride=2),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        X = self.embedding(X)\n",
    "        return self.conv_blocks(X)\n",
    "    \n",
    "\n",
    "test = FeatureExtractor(vocab_size=len_vocab,\n",
    "                        embedding_dim=100,\n",
    "                        seq_len=2969).to(device)\n",
    "X, _ = next(iter(test_dataloader))\n",
    "print(test(X).size())\n",
    "del X, test\n",
    "\n",
    "\n",
    "class EmotionalClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 embedding_dim: int,\n",
    "                 seq_len: int,\n",
    "                 output_size: int,\n",
    "                 hidden_layers: int):\n",
    "        super().__init__()\n",
    "        self.input_layer = FeatureExtractor(vocab_size,\n",
    "                                            embedding_dim,\n",
    "                                            seq_len)  # output shape: (batch_size, 256, 11)\n",
    "        self.hidden_layers = nn.LSTM(input_size=11,\n",
    "                                     hidden_size=128,\n",
    "                                     num_layers=hidden_layers,\n",
    "                                     batch_first=True)  # output shape: (batch_size, 2 * hidden_size)\n",
    "        self.flatten = nn.Flatten()\n",
    "        hidden_size = 256 * 128\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.states = self.init_states(hidden_layers, 128)\n",
    "        \n",
    "    def init_states(self, num_layers: int, hidden_size: int) -> tuple[torch.Tensor,\n",
    "                                                                      torch.Tensor]:\n",
    "        return (torch.zeros((num_layers, 16, hidden_size), device=device),\n",
    "                torch.zeros((num_layers, 16, hidden_size), device=device))\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        X = self.input_layer(X)\n",
    "        X, self.states = self.hidden_layers(X, self.states)\n",
    "        self.states = self.states[0].detach(), self.states[1].detach()\n",
    "        X = self.flatten(X)\n",
    "        return self.linear_layers(X)\n",
    "    \n",
    "\n",
    "model = EmotionalClassifier(vocab_size=len_vocab,\n",
    "                            embedding_dim=100,\n",
    "                            seq_len=2969,\n",
    "                            output_size=5,\n",
    "                            hidden_layers=1).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "critetion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def batch_stat(x: torch.Tensor) -> str:\n",
    "    predicted = x.argmax(dim=1).tolist()\n",
    "    dict_ = {key: predicted.count(key) for key in set(predicted)}\n",
    "    return str(dict_)\n",
    "\n",
    "\n",
    "mean_train_losses = []\n",
    "mean_test_losses = []\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    model.train(True)\n",
    "    for batch, (inputs, targets) in enumerate(train_dataloader):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = critetion(outputs, targets.argmax(dim=1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        if (batch + 1) % 5 == 0:\n",
    "            print(f'Epoch: {epoch + 1}, Train batch {batch + 1}, loss: {train_losses[-1]}\\noutputs {batch_stat(outputs)}')\n",
    "        if (batch + 1) % 50 == 0:\n",
    "            clear_output()\n",
    "\n",
    "    train_losses = np.array(train_losses)\n",
    "    mean_train_losses.append(train_losses.mean())\n",
    "    predicted = np.zeros(0)\n",
    "    true = np.zeros(0)\n",
    "    \n",
    "    clear_output()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, targets) in enumerate(test_dataloader):\n",
    "            outputs = model(inputs)\n",
    "            loss = critetion(outputs, targets)\n",
    "            \n",
    "            predicted = np.concatenate((predicted, outputs.argmax(dim=1).tolist()))\n",
    "            true = np.concatenate((true, targets.argmax(dim=1).tolist()))\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "            \n",
    "            print(f'Epoch: {epoch + 1}, Test batch {batch + 1}, loss: {test_losses[-1]}\\noutputs {batch_stat(outputs)}')\n",
    "        clear_output()\n",
    "        \n",
    "    test_losses = np.array(test_losses)\n",
    "    mean_test_losses.append(test_losses.mean())\n",
    "print(classification_report(true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "axes = fig.subplots(1, 2)\n",
    "axes[0].plot(np.arange(1, epochs + 1), np.array(mean_train_losses),\n",
    "         color=\"red\", label=\"Train\")\n",
    "axes[1].plot(np.arange(1, epochs + 1), np.array(mean_test_losses),\n",
    "         color=\"blue\", label=\"Test\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"CrossEntropy\")\n",
    "\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"CrossEntropy\")\n",
    "\n",
    "fig.suptitle(\"Losses\")\n",
    "axes[0].grid()\n",
    "axes[1].grid()\n",
    "fig.legend()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
